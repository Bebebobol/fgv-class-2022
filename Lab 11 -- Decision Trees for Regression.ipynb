{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be049f25",
   "metadata": {},
   "source": [
    "# Decision Trees for Regression\n",
    "\n",
    "Compared to last week, this is a very simple lab <span style=\"font-size:20pt;\">ðŸ˜ƒ</span>\n",
    "\n",
    "You will implement the [Classification and Regression Tree (CART)](https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29) algorithm from scratch.\n",
    "\n",
    "The lab is broken down into the following pieces:\n",
    "\n",
    "* Regression Criterion\n",
    "* Creating Splits\n",
    "* Buiding a Tree\n",
    "* Making a prediction\n",
    "\n",
    "## Exercise 1 -- Download and load the dataset\n",
    "\n",
    "This time, we will be using the banknote authentication data set, which is available to download from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/banknote+authentication)\n",
    "\n",
    "* Download the file\n",
    "* Read it and make a scikit-learn style dataset from it\n",
    "* Make a 70/30 train test split\n",
    "\n",
    "**TIP:** Pandas can readl URLs directly (But it's still nice to have the dataset locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "122e602d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcefe5-a770-465d-8ec6-93f233c21196",
   "metadata": {},
   "source": [
    "Make a dataset where the features are\n",
    "\n",
    "* `variance`\n",
    "* `skewness`\n",
    "* `curtosis`\n",
    "\n",
    "and the target variable is\n",
    "\n",
    "* `entropy`\n",
    "\n",
    "For this lab, simply ignore the `class` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e509581-9eb5-4653-9f57-bb79ad5a2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc019",
   "metadata": {},
   "source": [
    "## Exercise 2 -- Optimization Criterion\n",
    "\n",
    "For regression, a simple criterion to optimize is to minimize the sum of squared errors for a given region. This is, for all datapoints in a region with size, we minimize:\n",
    "\n",
    "$$\\sum_{i=1}^N(y_i - \\hat{y})^2$$\n",
    "\n",
    "where $N$ is the number of datapoits in the region and $\\hat{y}$ is the mean value of the region for the target variable. \n",
    "\n",
    "Implement such a function using the description below.\n",
    "\n",
    "Please, don't use an existing implementation, refer to the [book](https://www.statlearning.com/s/ISLRSeventhPrinting.pdf), and if you need help, as questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2452bbc-f673-4abc-8bfb-934d82d29033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_criterion(region):\n",
    "    \"\"\"\n",
    "    Implements the sum of squared error criterion in a region\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : ndarray\n",
    "        Array of shape (N,) containing the values of the target values \n",
    "        for N datapoints in the training set.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The sum of squared error\n",
    "        \n",
    "    Note\n",
    "    ----\n",
    "    The error for an empty region should be infinity\n",
    "    This avoids creating empty regions\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebc0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7733391199176196e-30\n",
      "0.0\n",
      "0.0\n",
      "inf\n"
     ]
    }
   ],
   "source": [
    "# test your code\n",
    "rng = np.random.default_rng(0)\n",
    "print(regression_criterion(rng.random(size=40)))\n",
    "print(regression_criterion(np.ones(10)))\n",
    "print(regression_criterion(np.zeros(10)))\n",
    "print(regression_criterion(np.array([])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8877dd90",
   "metadata": {},
   "source": [
    "## Exercise 3 -- Make a split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85e985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_region(region, feature_index, tau):\n",
    "    \"\"\"\n",
    "    Given a region, splits it based on the feature indicated by\n",
    "    `feature_index`, the region will be split in two, where\n",
    "    one side will contain all points with the feature with values \n",
    "    lower than `tau`, and the other split will contain the \n",
    "    remaining datapoints.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    region : array of size (n_samples, n_features)\n",
    "        a partition of the dataset (or the full dataset) to be split\n",
    "    feature_index : int\n",
    "        the index of the feature (column of the region array) used to make this partition\n",
    "    tau : float\n",
    "        The threshold used to make this partition\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    low_partition : array\n",
    "        indices of the datapoints in `region` where feature < `tau`\n",
    "    high_partition : array\n",
    "        indices of the datapoints in `region` where feature >= `tau` \n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff339070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324,) (636,)\n"
     ]
    }
   ],
   "source": [
    "r, l = split_region(X_train, 1, 0)\n",
    "print(r.shape, l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133c3de",
   "metadata": {},
   "source": [
    "## Exercise 4 -- Find the best split\n",
    "\n",
    "The strategy is quite simple (as well as inefficient), but it helps to reinforce the concepts.\n",
    "We are going to use a greedy, exhaustive algorithm to select splits, selecting the `feature_index` and the `tau` that minimizes the Regression Criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f87678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X, y):\n",
    "    \"\"\"\n",
    "    Given a dataset (full or partial), splits it on the feature of that minimizes the sum of squared error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array (n_samples, n_features)\n",
    "        features \n",
    "    y : array (n_samples, )\n",
    "        labels\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    decision : dictionary\n",
    "        keys are:\n",
    "        * 'feature_index' -> an integer that indicates the feature (column) of `X` on which the data is split\n",
    "        * 'tau' -> the threshold used to make the split\n",
    "        * 'low_region' -> array of indices where the `feature_index`th feature of X is lower than `tau`\n",
    "        * 'high_region' -> indices not in `low_region`\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a392f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_index': 0,\n",
       " 'tau': 0.6818,\n",
       " 'low_region': array([0, 1, 2, 4]),\n",
       " 'high_region': array([3])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_split(X_train[:5, :], y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3210e17",
   "metadata": {},
   "source": [
    "## Exercise 5 -- Recursive Splitting\n",
    "\n",
    "The test above is an example on how to find the root node of our decision tree. The algorithm now is a greedy search until we reach a stop criterion.\n",
    "\n",
    "The trivial stopping criterion is to recursively grow the tree until each split contains a single point (perfect node purity). If we go that far, it normally means we are overfitting.\n",
    "\n",
    "You will implement these criteria to stop the growth:\n",
    "\n",
    "* A node is a leaf if:\n",
    "    * It has less than `min_samples` datapoints\n",
    "    * It is at the `max_depth` level from the root (each split creates a new level)\n",
    "    * The criterion is `0`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6afcdf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_growth(node, min_samples, max_depth, current_depth, X, y):\n",
    "    \"\"\"\n",
    "    Recursively grows a decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        If the node is terminal, it contains only the \"value\" key, which determines the value to be used as a prediction.\n",
    "        If the node is not terminal, the dictionary has the structure defined by `get_split`\n",
    "    min_samples : int\n",
    "        parameter for stopping criterion if a node has <= min_samples datapoints\n",
    "    max_depth : int\n",
    "        parameter for stopping criterion if a node belongs to this depth\n",
    "    depth : int\n",
    "        current distance from the root\n",
    "    X : array (n_samples, n_features)\n",
    "        features (full dataset)\n",
    "    y : array (n_samples, )\n",
    "        labels (full dataset)\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    To create a terminal node, a dictionary is created with a single \"value\" key, with a value that\n",
    "    is the mean of the target variable\n",
    "    \n",
    "    'left' and 'right' keys are added to non-terminal nodes, which contain (possibly terminal) nodes \n",
    "    from higher levels of the tree:\n",
    "    'left' corresponds to the 'low_region' key, and 'right' to the 'high_region' key\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c0bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "test_root = get_split(X_train[:k, :], y_train[:k])\n",
    "recursive_growth(test_root, 5, 3, 1, X_train[:k, :], y_train[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcdfbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, depth):\n",
    "    if 'value' in node.keys():\n",
    "        print('.  '*(depth-1), f\"[{node['value']}]\")\n",
    "    else:\n",
    "        print('.  '*depth, f'X_{node[\"feature_index\"]} < {node[\"tau\"]}')\n",
    "        print_tree(node['left'], depth+1)\n",
    "        print_tree(node['right'], depth+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7192a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_2 < 0.91995\n",
      ".   X_0 < -2.0754\n",
      ".   [-2.268955]\n",
      ".  .   X_0 < -0.90784\n",
      ".  .   [-2.9952533333333338]\n",
      ".  .   [-2.9952533333333338]\n",
      ".   X_1 < -6.8804\n",
      ".   [-3.379095]\n",
      ".  .   X_2 < 0.37158\n",
      ".  .   [-1.899805]\n",
      ".  .   [-1.899805]\n"
     ]
    }
   ],
   "source": [
    "print_tree(test_root, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bb25f",
   "metadata": {},
   "source": [
    "# Exercise 6 -- Make a Prediction\n",
    "Use the a node to predict the class of a compatible dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2f31136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(node, sample):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    sample : array of size (n_features,)\n",
    "        a sample datapoint\n",
    "    \"\"\"\n",
    "    # your code here\n",
    "        \n",
    "def predict(node, X):\n",
    "    \"\"\"\n",
    "    Makes a prediction based on the decision tree defined by `node`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node : dictionary\n",
    "        A node created one of the methods above\n",
    "    X : array of size (n_samples, n_features)\n",
    "        n_samples predictions will be made\n",
    "    \"\"\"\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536a10f-bf8d-467b-b0bc-2ccbc31c08f2",
   "metadata": {},
   "source": [
    "Use the following node to test different parameters for the recursive growth. \n",
    "\n",
    "* What happens if you let the tree grow very deep?\n",
    "* What happens if the minimum number of samples is too big? What happens if it is 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b3da6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE : 4.582477460507463\n",
      "Test MSE : 4.704711300439497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "root = get_split(X_train, y_train)\n",
    "recursive_growth(root, 20, 6, 1, X_train, y_train)\n",
    "train_mse = mean_squared_error(y_train, predict(root, X_train))\n",
    "test_mse = mean_squared_error(y_test, predict(root, X_test))\n",
    "\n",
    "print(f'Train MSE : {train_mse}')\n",
    "print(f'Test MSE : {test_mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
